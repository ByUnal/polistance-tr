{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f575d4-fde3-40a0-ae10-01f12b7b837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 15:36:13.676505: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-16 15:36:13.680648: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-16 15:36:13.690208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-16 15:36:13.705675: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-16 15:36:13.710244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-16 15:36:13.722360: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 15:36:14.623041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "\n",
    "# Hugging face dataset import for data loading\n",
    "from datasets import Dataset\n",
    "\n",
    "# Sklearn for metric calculations and other preporcessing tasks\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6680f397-eb97-4524-9afe-45521a6e39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3e1e93-69fc-4f13-b808-0fc7abfc0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f11b02-3a00-4ddc-a4db-0042c0c2fe47",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dad7e29-2c73-46bd-a5db-bf2669195fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stopwords.words('turkish')])\n",
    "    # return \" \".join([word for word in stopword_remover.drop_stop_words(str(text).split())])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    puncs = '’!\"#$%&\\'*+:;<=>?...@[\\\\]^_`{|}~“”'\n",
    "\n",
    "    # Remove punctuation \n",
    "    return text.translate(str.maketrans('', '', puncs)) \n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = remove_punctuation(text)\n",
    "    \n",
    "    # text = remove_stopwords(text)\n",
    "    \n",
    "    # Remove digits\n",
    "    text = re.sub(r'[0-9]{2}', '', text)\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    text = text.translate(remove_digits)\n",
    "    \n",
    "    text = re.sub(' +', ' ', text) # remove extra whitespaces\n",
    "    text = re.sub(r'([^\\w\\s])\\1+', r'\\1', text)\n",
    "    text = re.sub(r'\\s?([^\\w\\s])\\s?', ' ', text)\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cdfd116-2f72-4490-a17a-1652fe420536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((6059, 2), (674, 2), (1189, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/stance_train.csv\")\n",
    "df_val = pd.read_csv(\"data/stance_val.csv\")\n",
    "df_test = pd.read_csv(\"data/stance_test.csv\")\n",
    "\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Check how many labels are there in the dataset\n",
    "unique_labels = df_train.labels.unique().tolist()\n",
    "\n",
    "# Map each label into its id representation and vice versa\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "print(ids_to_labels)\n",
    "\n",
    "df_train['labels'] = df_train.labels.apply(lambda x: labels_to_ids[x]).tolist()\n",
    "df_val['labels'] = df_val.labels.apply(lambda x: labels_to_ids[x]).tolist()\n",
    "df_test['labels'] = df_test.labels.apply(lambda x: labels_to_ids[x]).tolist()\n",
    "\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c3532f-3bdf-45aa-a5a8-0e541ff7c42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "2    2236\n",
       "0    2181\n",
       "1    1642\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0783109-fed0-4abd-b8f4-b815d131f3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ben bunu tam gün önce paylaştım daha yeni duym...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harita teknikerleri olarak aldığımız yüksek pu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bakan özhaseki imar barışının detaylarını açık...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allah isinizirasgetirsin allah senden raraziolsun</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sayın bakanım biz ek gösterge mağdurlarıyız ta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  ben bunu tam gün önce paylaştım daha yeni duym...       0\n",
       "1  harita teknikerleri olarak aldığımız yüksek pu...       1\n",
       "2  bakan özhaseki imar barışının detaylarını açık...       1\n",
       "3  allah isinizirasgetirsin allah senden raraziolsun       2\n",
       "4  sayın bakanım biz ek gösterge mağdurlarıyız ta...       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7492d08-3c32-4448-8895-18a6ee8d8272",
   "metadata": {},
   "source": [
    "# Check Token Coverage\n",
    "\n",
    "* The decision to weather we shall go for a pretrained tokenizer or shall we retrain the tokenizer can be based on tokens coverage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc1def3-d407-412f-b340-90b17ea718ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of tokens unknown: 0.0\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = 'dbmdz/bert-base-turkish-cased'\n",
    "# MODEL_NAME = 'dbmdz/distilbert-base-turkish-cased'\n",
    "# MODEL_NAME = 'dbmdz/convbert-base-turkish-cased'\n",
    "# MODEL_NAME = 'dbmdz/electra-base-turkish-cased-discriminator'\n",
    "# MODEL_NAME = 'loodos/albert-base-turkish-uncased'\n",
    "# MODEL_NAME = 'burakaytan/roberta-base-turkish-uncased'\n",
    "MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MAX_LEN)\n",
    "\n",
    "total_token_count = 0\n",
    "unk_token_count = 0\n",
    "\n",
    "for index, dp in df_train.iterrows():\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(str(dp['text']))\n",
    "    unk_token_count += len([i for i in tokenized_text if i[0:2] == \"##\"])\n",
    "    total_token_count += len(tokenized_text)\n",
    "\n",
    "print (f\"Percentage of tokens unknown: {(100.0 * unk_token_count/total_token_count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b0ab7-3397-4bed-bb51-bdd142ca3c71",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "* Load data into Dataset class, and tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c50bd51-b039-4ff3-9094-72f31c9ece7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 17:01:29.749387: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Map (num_proc=2): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6059/6059 [00:01<00:00, 3668.82 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 674/674 [00:00<00:00, 3855.61 examples/s]\n",
      "Map (num_proc=2): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1189/1189 [00:00<00:00, 1286.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from vnlp import Normalizer\n",
    "def lower_case_func(text):\n",
    "    return Normalizer.lower_case(text)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length = MAX_LEN, return_tensors='pt')\n",
    "\n",
    "if \"uncased\" in MODEL_NAME:\n",
    "    df_train['text'] = df_train.text.apply(lower_case_func).tolist()\n",
    "    df_val['text'] = df_val.text.apply(lower_case_func).tolist()\n",
    "    df_test['text'] = df_test.text.apply(lower_case_func).tolist()\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train[[\"text\", \"labels\"]], split=\"train\")\n",
    "dataset_val = Dataset.from_pandas(df_val[[\"text\", \"labels\"]], split=\"val\")\n",
    "dataset_test = Dataset.from_pandas(df_test[[\"text\", \"labels\"]], split=\"test\")\n",
    "\n",
    "train_dataset = dataset_train.map(preprocess_function, batched=True, num_proc=2, remove_columns=[\"text\"], fn_kwargs={\"tokenizer\":tokenizer})\n",
    "val_dataset = dataset_val.map(preprocess_function, batched=True, remove_columns=[\"text\"], fn_kwargs={\"tokenizer\":tokenizer})\n",
    "test_dataset = dataset_test.map(preprocess_function, batched=True, num_proc=2, remove_columns=[\"text\"], fn_kwargs={\"tokenizer\":tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de96a92-5fc9-47c4-add1-e4e591b205f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ben bunu tam gün önce paylaştım daha yeni duym...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harita teknikerleri olarak aldığımız yüksek pu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bakan özhaseki imar barışının detaylarını açık...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allah isinizirasgetirsin allah senden raraziolsun</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sayın bakanım biz ek gösterge mağdurlarıyız ta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  ben bunu tam gün önce paylaştım daha yeni duym...       0\n",
       "1  harita teknikerleri olarak aldığımız yüksek pu...       1\n",
       "2  bakan özhaseki imar barışının detaylarını açık...       1\n",
       "3  allah isinizirasgetirsin allah senden raraziolsun       2\n",
       "4  sayın bakanım biz ek gösterge mağdurlarıyız ta...       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac3b48-7101-446f-a5ee-cf8736e4b6bc",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a06c5e8-51ee-4cab-9912-826797dfb1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    \n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b3398d-81c1-4428-a2db-a8c496f644d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,\n",
    "                                                           num_labels=len(unique_labels))\n",
    "\n",
    "EPOCH = 10\n",
    "warmup_steps = math.ceil(len(train_dataset) * EPOCH * 0.1)\n",
    "BATCH_SIZE = 32\n",
    "LR = 5e-5\n",
    "WD = 0.003\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=EPOCH,\n",
    "    \n",
    "    # Optimizer Hyperparameters\n",
    "    optim = \"adamw_torch\",\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WD,\n",
    "    warmup_steps=warmup_steps,\n",
    "    \n",
    "    # Logging Hyperparameters\n",
    "    run_name=\"stance-detection\",\n",
    "    output_dir=\"stance-chkpt\",\n",
    "    overwrite_output_dir=True,\n",
    "    logging_steps=250,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    \n",
    "    # Wieght and Biases\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # General Hyperparameters\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    save_total_limit=1,\n",
    "    # gradient_checkpointing=True,\n",
    "    do_train=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b42eed7-a4ee-452f-9ff0-03a7763a7f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1900' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1900/1900 31:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.074900</td>\n",
       "      <td>0.940950</td>\n",
       "      <td>0.586053</td>\n",
       "      <td>0.461221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>0.623116</td>\n",
       "      <td>0.744807</td>\n",
       "      <td>0.731135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.621300</td>\n",
       "      <td>0.585877</td>\n",
       "      <td>0.777448</td>\n",
       "      <td>0.766948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.519700</td>\n",
       "      <td>0.587895</td>\n",
       "      <td>0.783383</td>\n",
       "      <td>0.775567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.453400</td>\n",
       "      <td>0.605587</td>\n",
       "      <td>0.790801</td>\n",
       "      <td>0.786615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.574718</td>\n",
       "      <td>0.793769</td>\n",
       "      <td>0.790433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.703417</td>\n",
       "      <td>0.778932</td>\n",
       "      <td>0.769910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1900, training_loss=0.5682379833020662, metrics={'train_runtime': 1903.6362, 'train_samples_per_second': 31.829, 'train_steps_per_second': 0.998, 'total_flos': 1.594204198013952e+16, 'train_loss': 0.5682379833020662, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aead27c3-8dff-4ad6-9768-23c0ab8adef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for FacebookAI/xlm-roberta-base\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss = 0.5669955015182495\n",
      "eval_accuracy = 0.8031959629941127\n",
      "eval_f1 = 0.7986781397182857\n",
      "eval_runtime = 11.6316\n",
      "eval_samples_per_second = 102.222\n",
      "eval_steps_per_second = 3.267\n",
      "epoch = 10.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Results for {MODEL_NAME}\")\n",
    "results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "for key, value in results.items():\n",
    "    print(f\"{key} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01eacc8f-c922-4dbd-a165-31d896f67213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trained-models/xlm-roberta-base'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # SAVE MODEL\n",
    "prefix = MODEL_NAME.replace(\"dbmdz/\",\"\").replace(\"loodos/\",\"\").replace(\"burakaytan/\",\"\").replace(\"FacebookAI/\",\"\")\n",
    "save_path = f\"trained-models/{prefix}\"\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1537fc5-191f-4ab6-9b0f-7673fb883cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f'{save_path}')\n",
    "# Save Parameters\n",
    "with open(f\"{save_path}/parameters.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"MODEL NAME: {MODEL_NAME}\\n\")\n",
    "    f.write(f\"MAX LEN: {MAX_LEN}\\n\")\n",
    "    f.write(f\"EPOCH: {EPOCH}\\n\")\n",
    "    f.write(f\"BATCH SIZE: {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"LR: {LR}\\n\")\n",
    "    f.write(f\"WD: {WD}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1b28b80-4aaf-4b62-85ec-fc83cf357ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{save_path}/id2label.json\", \"w+\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(ids_to_labels, fp, indent=4)\n",
    "\n",
    "with open(f\"{save_path}/label2id.json\", \"w+\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(labels_to_ids, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2d579-eba0-4235-a574-fe9d38b67472",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8464e72f-e901-4ebb-96e5-290764fb4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\"\n",
    "def load_model_and_tokenizer(model_path, num_labels, max_len, device=\"cuda\"):\n",
    "    # LOAD MODEL AND TOKENIZER\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
    "    model = model.to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, model_max_length=max_len)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c2ea92-92b6-4ba1-adc8-279fbf347b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['dbmdz/bert-base-turkish-cased', 'dbmdz/distilbert-base-turkish-cased', 'dbmdz/convbert-base-turkish-cased', \n",
    "               'dbmdz/electra-base-turkish-cased-discriminator', 'loodos/albert-base-turkish-uncased', \n",
    "               'burakaytan/roberta-base-turkish-uncased', 'FacebookAI/xlm-roberta-base']\n",
    "\n",
    "for mn in model_names:\n",
    "    # # SAVE MODEL\n",
    "    prefix = mn.replace(\"dbmdz/\",\"\").replace(\"loodos/\",\"\").replace(\"burakaytan/\",\"\").replace(\"FacebookAI/\",\"\")\n",
    "    load_path = f\"trained-models/{prefix}\"\n",
    "    print(load_path)\n",
    "\n",
    "    df_train = pd.read_csv(\"data/stance_train.csv\")\n",
    "    df_val = pd.read_csv(\"data/stance_val.csv\")\n",
    "    df_test = pd.read_csv(\"data/stance_test.csv\")\n",
    "    \n",
    "    MAX_LEN = 512\n",
    "    \n",
    "    # Check how many labels are there in the dataset\n",
    "    unique_labels = df_train.labels.unique().tolist()\n",
    "    \n",
    "    # Map each label into its id representation and vice versa\n",
    "    labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "    \n",
    "    df_train['labels'] = df_train.labels.apply(lambda x: labels_to_ids[x]).tolist()\n",
    "    df_val['labels'] = df_val.labels.apply(lambda x: labels_to_ids[x]).tolist()\n",
    "    df_test['labels'] = df_test.labels.apply(lambda x: labels_to_ids[x]).tolist()\n",
    "\n",
    "    if \"uncased\" in mn:\n",
    "        df_train['text'] = df_train.text.apply(lower_case_func).tolist()\n",
    "        df_val['text'] = df_val.text.apply(lower_case_func).tolist()\n",
    "        df_test['text'] = df_test.text.apply(lower_case_func).tolist()\n",
    "\n",
    "    dataset_train = Dataset.from_pandas(df_train[[\"text\", \"labels\"]], split=\"train\")\n",
    "    dataset_val = Dataset.from_pandas(df_val[[\"text\", \"labels\"]], split=\"val\")\n",
    "    dataset_test = Dataset.from_pandas(df_test[[\"text\", \"labels\"]], split=\"test\")\n",
    "\n",
    "    tokenizer, loaded_model = load_model_and_tokenizer(load_path, len(labels_to_ids), MAX_LEN)\n",
    "\n",
    "    train_dataset = dataset_train.map(preprocess_function, batched=True, num_proc=2, remove_columns=[\"text\"], fn_kwargs={\"tokenizer\":tokenizer})\n",
    "    val_dataset = dataset_val.map(preprocess_function, batched=True, remove_columns=[\"text\"], fn_kwargs={\"tokenizer\":tokenizer})\n",
    "    test_dataset = dataset_test.map(preprocess_function, batched=True, num_proc=2, remove_columns=[\"text\"], fn_kwargs={\"tokenizer\":tokenizer})\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    with open(\"./stance_detection_results\", \"a+\") as sdr:\n",
    "        sdr.write(f\"Results for: {mn}\\n\")\n",
    "        results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        sdr.write(f\"Accuracy = {results['eval_accuracy']:.2f}\\n\")\n",
    "        sdr.write(f\"Macro F1 = {results['eval_f1']:.2f}\\n\")\n",
    "        sdr.write(\"*\"*60)\n",
    "        sdr.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "245fb8b3-935f-4b0a-8e67-083556e8ef5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trained-models/electra-base-turkish-cased-discriminator'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn = 'dbmdz/electra-base-turkish-cased-discriminator'\n",
    "\n",
    "df_train = pd.read_csv(\"data/stance_train.csv\")\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Check how many labels are there in the dataset\n",
    "unique_labels = df_train.labels.unique().tolist()\n",
    "\n",
    "# Map each label into its id representation and vice versa\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "# ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "\n",
    "# # SAVE MODEL\n",
    "prefix = mn.replace(\"dbmdz/\",\"\").replace(\"loodos/\",\"\").replace(\"burakaytan/\",\"\").replace(\"FacebookAI/\",\"\")\n",
    "load_path = f\"trained-models/{prefix}\"\n",
    "load_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d7d59af-0b93-4437-9489-a541405ab074",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, loaded_model = load_model_and_tokenizer(load_path, len(labels_to_ids), MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78bdd0ba-c805-4b77-88fe-394a3951480b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/byunal/electra-base-turkish-cased-stance/commit/28d8c50e179747fa4a68f94a46dc9b9ccf9fe37a', commit_message='Upload tokenizer', commit_description='', oid='28d8c50e179747fa4a68f94a46dc9b9ccf9fe37a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"byunal/electra-base-turkish-cased-stance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9b9cf04-29fd-46f5-accb-1b67912bb30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894db97c020a4debb51ce02da32a7447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d3e5ffeb3b4f2b9b5318669d39c344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/byunal/electra-base-turkish-cased-stance/commit/24e2823093c884c8484f2739fc3721a5269f4ba7', commit_message='Upload ElectraForSequenceClassification', commit_description='', oid='24e2823093c884c8484f2739fc3721a5269f4ba7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.push_to_hub(\"byunal/electra-base-turkish-cased-stance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa0be3-c4b5-4e7f-bb59-eaab839f77b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cht",
   "language": "python",
   "name": "cht"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
